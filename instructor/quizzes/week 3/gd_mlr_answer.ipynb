{
 "cells": [
  {
   "cell_type": "raw",
   "id": "310cc7dd-491a-44f7-8851-0a3861c61e8f",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Quiz 2\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92485c6b-75a4-4e2a-8794-84338e0db8bc",
   "metadata": {},
   "source": [
    "# Gradient Descent with Multiple Linear Regression Using For Loop\n",
    "In this lab, you'll extend simple linear regression to handle multiple input features and explore methods for enhancing your model's training and performance, including vector operation in NumPy and feature scaling. The lab consists of 6 tasks, during which you'll practice numpy vector operation and implementing gradient descent for multiple linear regression using for loops in your code.\n",
    "\n",
    "**After finishing the tasks, please take the corresponding quiz on canvas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c8464-0e4d-4094-8c3a-94c63feed394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use('./deeplearning.mplstyle')\n",
    "np.set_printoptions(precision=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118779ce-9bce-4a74-9017-02bd84ddb7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data = np.loadtxt(\"Datasets/houses.txt\", delimiter=',')\n",
    "X_train, y_train = data[:,:-1], data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b244ae-6503-440b-ac0c-a777aae7019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1c2cf-5411-446d-ad6c-3f4fe97a207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is stored in numpy array/matrix\n",
    "print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b315340-f5fc-4406-b634-7ce8054c91fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae34961b-cd4b-46e9-9842-faf47ba20a8c",
   "metadata": {},
   "source": [
    "# 1. Model Prediction With Multiple Variables\n",
    "The model's prediction with multiple variables is given by the multiple linear model:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "or in vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "where $\\cdot$ is a vector `dot product`\n",
    "\n",
    "To demonstrate the dot product, we will implement prediction using (1) and (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cfa9f0-0aa6-47ec-b810-674ceff9bb42",
   "metadata": {},
   "source": [
    "### 1.1 Single Prediction, loop\n",
    "Our previous prediction multiplied one feature value by one parameter and added a bias parameter. A direct extension of our previous implementation of prediction to multiple features would be to implement (1) above using loop over each feature, performing the multiply with its corresponding parameter and then adding the bias parameter at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75700b9-0e9e-491f-8da3-836705922afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_loop(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray): Shape (m,) example with multiple features\n",
    "      w (ndarray): Shape (m,) model parameters, each feature has a weight/parameter    \n",
    "      b (scalar):  model bias parameter     \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    m = x.shape[0]\n",
    "    p = 0\n",
    "    for i in range(m):\n",
    "        p_i = x[i] * w[i]  \n",
    "        p = p + p_i         \n",
    "    p = p + b                \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305051d7-a634-49dc-997b-b57f234f2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a row from our training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict_single_loop(x_vec, w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e911fe-22b1-4e81-8413-ad063a6e131f",
   "metadata": {},
   "source": [
    "Note the shape of `x_vec`. It is a 1-D NumPy vector with 4 elements, (4,). The result, `f_wb` is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef0a913-c24e-409d-86c7-8587521bb24a",
   "metadata": {},
   "source": [
    "## 1.2 Single Prediction, vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2613f470-db91-48d2-9126-aa8f96754375",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Noting that equation (1) above can be implemented using the dot product as in (2) above. We can make use of vector operations to simplify predictions.\n",
    "\n",
    "Recall from the Python/Numpy lab that NumPy `np.dot()`[[link](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)] can be used to perform a vector dot product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f211bf-aa75-457b-bc20-2d422a2eb4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (m,) example with multiple features\n",
    "      w (ndarray): Shape (m,) model parameters   \n",
    "      b (scalar):             model parameter \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b     \n",
    "    return p   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e2331d-1cbc-4bf7-ba97-a9350ceb358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a row from our training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict(x_vec,w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b24cb5-156a-46ae-972a-b174e758d795",
   "metadata": {},
   "source": [
    "The results and shapes are the same as the previous version which used looping. Going forward, `np.dot` will be used for these operations. The prediction is now a single statement. Most routines will implement it directly rather than calling a separate predict routine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f81dea2-f84d-497f-ac18-f5fbddd466a3",
   "metadata": {},
   "source": [
    "# 2. Gradient Descent for Multiple Linear Regression\n",
    "You will implement gradient descent algorithm for multiple features. As in the last lab, you will need three functions.\n",
    "- `compute_cost`\n",
    "- `compute_gradient`\n",
    "- `gradient_descent`\n",
    "\n",
    "Conventions:\n",
    "- The naming of python variables containing partial derivatives follows this pattern,$\\frac{\\partial J(w,b)}{\\partial b}$  will be `dj_db`.\n",
    "- w.r.t is With Respect To, as in partial derivative of $J(w, b)$ With Respect To $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad29424-8255-4e6c-b0e6-68564264f2a7",
   "metadata": {},
   "source": [
    "## 2.1. Compute Cost With Multiple Variables\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "\n",
    "In contrast to the last lab, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a5cb57-a741-4af2-b992-01f73520f1eb",
   "metadata": {},
   "source": [
    "Below is an implementation of equations (3) and (4). Note that this uses a *standard pattern for this course* where a for loop over all `m` examples is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eea2b4-3a6c-4abd-8cae-ec3f3d02bab8",
   "metadata": {},
   "source": [
    "### Task 1: Fill in the code\n",
    "Implement the `compute_cost` below to compute the cost $J(w,b)$.\n",
    "\n",
    "Note that you need to use the dot product for single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc03de-c280-45f5-8b12-69c9d6837206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model weight parameters  \n",
    "      b (scalar)       : model bias parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    m = \n",
    "    cost = 0.0\n",
    "    for i in :                                \n",
    "        f_wb_i =             #(n,)(n,) = scalar (see np.dot)\n",
    "        cost =                          #scalar\n",
    "    cost =                       #scalar  \n",
    "    ### END CODE HERE ###\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d3e8f-d533-46c9-ba86-cbdcde46aa92",
   "metadata": {},
   "source": [
    "Test your implementation and take the quiz on canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88744ddc-ad83-4cb2-9f92-cbe2611c09ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute and display cost using our pre-chosen optimal parameters. \n",
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w : {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61792238-4338-42ad-974d-5f133f4199aa",
   "metadata": {},
   "source": [
    "## 2.2. Implement Gradient Descent With Multiple Variable Using Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef577a8-64c0-4897-afd3-f407afd0c662",
   "metadata": {},
   "source": [
    "\n",
    "Gradient descent for multiple variables:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* m is the number of training examples on the data set\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction for the $i$ th example, while $y^{(i)}$ is the corresponding target value\n",
    "\n",
    "The routine below implements equation (5) above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f221d0-75ab-46a8-bdce-8e42005183a5",
   "metadata": {},
   "source": [
    "### Task 2: Implement the `compute_gradient` function with Multiple Variables\n",
    "An implementation for calculating the equations (6) and (7) is below. There are many ways to implement this. In this version, you will be asked to use for loops, specifically:\n",
    "- outer loop over all m examples. \n",
    "    - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ for the example can be computed directly and accumulated\n",
    "    - in a second loop over all n features:\n",
    "        - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$ is computed for each $w_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1783bf-5c9e-4f44-9805-f4676aea57ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    m,n =            #(number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range():\n",
    "        # you need to calculate the error for each observation\n",
    "        err =    \n",
    "        for j in range():                         \n",
    "            dj_dw[j] =     \n",
    "        dj_db =                         \n",
    "    dj_dw =                                 \n",
    "    dj_db =                                 \n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8958da6f-da3d-444f-bdfa-ae0426f3a73c",
   "metadata": {},
   "source": [
    "You can check if your implementation was correct by running the following test code: Please fill in the blanks in the Canvas quiz accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a8b89f-17ab-49e6-af6c-1672ab8de70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute and display gradient \n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af78c2ac-c11e-4edd-ac27-607ae3db2d15",
   "metadata": {},
   "source": [
    "### Task 3: Implement the `gradient_descent` function with Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb9223-0b9f-48b2-afb0-13b5cfddb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range():\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db,dj_dw =    ##None\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w =                ##None\n",
    "        b =                ##None\n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( )\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "    return w, b, J_history #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef25350-b20c-436d-aef6-68fb90091c5c",
   "metadata": {},
   "source": [
    "In the next cell you will test the implementation, please take the Canvas quiz accordingly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38407f5a-866a-48b6-944b-20a9e06cc211",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 5.0e-7\n",
    "# run gradient descent \n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f}, {w_final} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd611c5-fbc9-42ed-b66f-bc9b2207080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cost versus iteration  \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "ax1.plot(J_hist)\n",
    "start=4000\n",
    "ax2.plot(start + np.arange(len(J_hist[start:])), J_hist[start:])\n",
    "ax1.set_title(\"Cost vs. iteration\");  ax2.set_title(\"Cost vs. iteration (tail)\")\n",
    "ax1.set_ylabel('Cost')             ;  ax2.set_ylabel('Cost') \n",
    "ax1.set_xlabel('iteration step')   ;  ax2.set_xlabel('iteration step') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b876d-3f98-4ee8-af18-558a2636e932",
   "metadata": {},
   "source": [
    "*These results are not inspiring*! Cost is still declining and our predictions are not very accurate. Let's next explore how to improve on this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9980fe-495b-4cb4-a7e3-b39c44bf5dfd",
   "metadata": {},
   "source": [
    "# 3. Feature Scaling in Gradient Descent\n",
    "\n",
    "Let's view the dataset and its features by plotting each feature versus price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e310d7b-0ad5-4e6f-8546-d104631c01f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_features = ['size(sqft)','bedrooms','floors','age']\n",
    "fig,ax=plt.subplots(1, 4, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:,i],y_train)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"Price (1000's)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e543682-f4cd-41b2-b22a-41c6e6ad07c6",
   "metadata": {},
   "source": [
    "As illustrated above, the features exhibit varying scales, a factor that holds significant importance in the gradient descent optimization algorithms, particularly when features span disparate orders of magnitude. Effective feature scaling is critical to ensure optimal convergence and performance. Two commonly used ways for feature scaling include: \n",
    " [Standard Scaling](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) (also called z-score normalization or standardization) and [MinMax Scaling](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf7d8be-c979-4822-bdfc-85288cd5e8fe",
   "metadata": {},
   "source": [
    "\n",
    "### z-score normalization \n",
    "After z-score normalization, all features will have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "To implement z-score normalization, adjust your input values as shown in this formula:\n",
    "$$x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j} \\tag{8}$$ \n",
    "where $j$ selects a feature or a column in the $\\mathbf{X}$ matrix. $µ_j$ is the mean of all the values for feature (j) and $\\sigma_j$ is the standard deviation of feature (j).\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\tag{9}\\\\\n",
    "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  \\tag{10}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    ">**Implementation Note:** When normalizing the features, it is important\n",
    "to store the values used for normalization - the mean value and the standard deviation used for the computations. After learning the parameters\n",
    "from the model, we often want to predict the prices of houses we have not\n",
    "seen before. Given a new x value (living room area and number of bed-\n",
    "rooms), we must first normalize x using the mean and standard deviation\n",
    "that we had previously computed from the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ed9b9f-251d-4e9d-909a-3fac1112ec4f",
   "metadata": {},
   "source": [
    "### Task 4: Implementation the `zscore_normalize_features` function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6393d-90b1-42f0-a633-0a13e58c0e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    computes  X, zcore normalized by column\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))     : input data, m examples, n features\n",
    "      \n",
    "    Returns:\n",
    "      X_norm (ndarray (m,n)): input normalized by column\n",
    "      mu (ndarray (n,))     : mean of each feature\n",
    "      sigma (ndarray (n,))  : standard deviation of each feature\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    # find the mean of each column/feature\n",
    "    mu     =                  # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  =                   # sigma will have shape (n,)\n",
    "    # normalize features on element-wise\n",
    "    X_norm = \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return (X_norm, mu, sigma)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb317e7-28da-47da-b1e9-94059e6e7c44",
   "metadata": {},
   "source": [
    "Let's use your implementation to normalize the data and compare it to the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce7be4e-9931-41c4-b5e1-465ba28aaaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the original features\n",
    "X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "print(f\"X_mu = {X_mu}, \\nX_sigma = {X_sigma}\")\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1c09e0-d591-4380-a337-e85534088d89",
   "metadata": {},
   "source": [
    "### Task 5: use the `StandardScaler` in Sklearn to check your implementation above and see whether you can get the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93830020-d84b-4321-b3b3-2d67ce4cdd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please code below, ensure that the variable names match the printed information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c67b0-f0cd-4542-9802-85350a213717",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_mu_sklearn = {X_mu_sklearn}, \\nX_sigma_sklearn = {X_sigma_sklearn}\")\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df0ed40-93b1-440a-a0c0-c1af7e80f581",
   "metadata": {},
   "source": [
    "After feature scaling with z-score normalization, let's visualize the features again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b481a7-3d58-4974-965e-a60e436152e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu     = np.mean(X_train,axis=0)   \n",
    "sigma  = np.std(X_train,axis=0) \n",
    "X_mean = (X_train - mu)\n",
    "X_norm = (X_train - mu)/sigma      \n",
    "\n",
    "fig,ax=plt.subplots(1, 3, figsize=(12, 3))\n",
    "ax[0].scatter(X_train[:,0], X_train[:,3])\n",
    "ax[0].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[0].set_title(\"unnormalized\")\n",
    "ax[0].axis('equal')\n",
    "\n",
    "ax[1].scatter(X_mean[:,0], X_mean[:,3])\n",
    "ax[1].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[1].set_title(r\"X - $\\mu$\")\n",
    "ax[1].axis('equal')\n",
    "\n",
    "ax[2].scatter(X_norm[:,0], X_norm[:,3])\n",
    "ax[2].set_xlabel(X_features[0]); ax[0].set_ylabel(X_features[3]);\n",
    "ax[2].set_title(r\"Z-score normalized\")\n",
    "ax[2].axis('equal')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "fig.suptitle(\"distribution of features before, during, after normalization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b439e3e-e745-4b0a-82d1-f3f978a9fd0e",
   "metadata": {},
   "source": [
    "The plot above shows the relationship between two of the training set parameters, \"age\" and \"size(sqft)\". *These are plotted with equal scale*. \n",
    "- Left: Unnormalized: The range of values or the variance of the 'size(sqft)' feature is much larger than that of age\n",
    "- Middle: The first step removes the mean or average value from each feature. This leaves features that are centered around zero. It's difficult to see the difference for the 'age' feature, but 'size(sqft)' is clearly around zero.\n",
    "- Right: The second step divides by the standard deviation. This leaves both features centered at zero with a similar scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1209b68-03d6-4b0e-9ce7-9048419a402d",
   "metadata": {},
   "source": [
    "After feature scaling, the peak to peak range of each column is reduced from a factor of thousands to a factor of 2-3 by normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f3d96b-e01b-4a44-b1bc-1565a8e16033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "def norm_plot(ax, data):\n",
    "    scale = (np.max(data) - np.min(data))*0.2\n",
    "    x = np.linspace(np.min(data)-scale,np.max(data)+scale,50)\n",
    "    _,bins, _ = ax.hist(data, x, color=\"xkcd:azure\")\n",
    "    #ax.set_ylabel(\"Count\")\n",
    "    \n",
    "    mu = np.mean(data); \n",
    "    std = np.std(data); \n",
    "    dist = norm.pdf(bins, loc=mu, scale = std)\n",
    "    \n",
    "    axr = ax.twinx()\n",
    "    axr.plot(bins,dist, color = \"orangered\", lw=2)\n",
    "    axr.set_ylim(bottom=0)\n",
    "    axr.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be69a1-b567-4518-8ebf-69c18126ed92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1, 4, figsize=(12, 3))\n",
    "for i in range(len(ax)):\n",
    "    norm_plot(ax[i],X_train[:,i],)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"count\");\n",
    "fig.suptitle(\"distribution of features before normalization\")\n",
    "plt.show()\n",
    "fig,ax=plt.subplots(1,4,figsize=(12,3))\n",
    "for i in range(len(ax)):\n",
    "    norm_plot(ax[i],X_norm[:,i],)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"count\"); \n",
    "fig.suptitle(\"distribution of features after normalization\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f16424-2a60-4173-9852-ec5b15e09345",
   "metadata": {},
   "source": [
    "### Task 6: let's run gradient_descent withe the normalized data and see whether it can help the algorithm converge at a better place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b11fbd-866a-4dd8-888c-a44aa7e1aad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 1.0e-3\n",
    "# run gradient descent \n",
    "w_norm_final, b_norm_final, J_norm_hist = gradient_descent(X_norm, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_norm_final:0.2f},{w_norm_final} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab25be-a0ff-4628-87dd-190236e5a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cost versus iteration  \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "ax1.plot(J_norm_hist)\n",
    "start=4000\n",
    "ax2.plot(start + np.arange(len(J_norm_hist[start:])), J_norm_hist[start:])\n",
    "ax1.set_title(\"Cost vs. iteration\");  ax2.set_title(\"Cost vs. iteration (tail)\")\n",
    "ax1.set_ylabel('Cost')             ;  ax2.set_ylabel('Cost') \n",
    "ax1.set_xlabel('iteration step')   ;  ax2.set_xlabel('iteration step') \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64cf6071-073c-4a04-993d-091d4cc8bf81",
   "metadata": {},
   "source": [
    "As evident from the above learning curve, the reduction in training cost accelerated notably after feature scaling. This enhancement underscores the pivotal role of feature scaling in expediting the convergence of gradient descent algorithms. Without proper scaling, these algorithms often endure oscillations, significantly delaying their journey towards the minimum point. Thus, feature scaling proves indispensable, particularly when dealing with features of varying orders of magnitude.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0938e661",
   "metadata": {},
   "source": [
    "### The importance of Feature scaling in gradient descent based ML algorithms:\n",
    "\n",
    "Normalized data can significantly improve the performance of gradient descent for several reasons:\n",
    "\n",
    "* **Faster Convergence**: Normalization ensures that all features contribute equally to the distance calculations, which helps the algorithm converge more quickly. If features have different scales, the gradient descent process may oscillate or take longer to reach the minimum.\n",
    "\n",
    "* **Avoiding Local Minima**: When data is not normalized, the optimization landscape can become skewed, making it harder for gradient descent to navigate. Normalized data helps create a more uniform landscape, reducing the likelihood of getting stuck in local minima.\n",
    "\n",
    "* **Improved Numerical Stability**: Normalization reduces the risk of numerical instability that can occur when working with large or small values. This stability is crucial for maintaining the precision of calculations during the optimization process.\n",
    "\n",
    "* **Better Interpretability**: Normalizing the data helps in understanding feature importance and the impact of each feature on the model's predictions. This can aid in feature selection and model refinement.\n",
    "\n",
    "By scaling your data to a standard range (like 0 to 1 or a mean of 0 with a standard deviation of 1), you can leverage these benefits to enhance the effectiveness of gradient descent in training your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ada253-e6f5-471b-bbff-7d2914e0891d",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "https://www.deeplearning.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9e4c1b-a4cf-480a-8efb-9ba3adba5bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
