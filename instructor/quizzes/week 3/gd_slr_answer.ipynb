{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b7586d88-1b36-42f6-9cf4-2d01cf6c5204",
   "metadata": {
    "id": "b7586d88-1b36-42f6-9cf4-2d01cf6c5204"
   },
   "source": [
    "---\n",
    "title: \"Quiz 1\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ce902-5c59-4eee-88a4-513fc4f49906",
   "metadata": {
    "id": "c18ce902-5c59-4eee-88a4-513fc4f49906"
   },
   "source": [
    "# Gradient Descent with Simple Linear Regression Using For Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a8da5-27b2-4eef-84a0-70a354fcb50b",
   "metadata": {
    "id": "038a8da5-27b2-4eef-84a0-70a354fcb50b"
   },
   "source": [
    "## Tools\n",
    "In this lab, we will make use of:\n",
    "- NumPy, a popular library for scientific computing\n",
    "- Matplotlib, a popular library for plotting data\n",
    "- plotting routines in the lab_utils.py file in the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6cc0416-20b6-4d53-88ce-b45b6f0a4acb",
   "metadata": {
    "id": "a6cc0416-20b6-4d53-88ce-b45b6f0a4acb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "153d9384-e918-4346-b704-fe59b219b57c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "153d9384-e918-4346-b704-fe59b219b57c",
    "outputId": "dce00b22-ea03-4f04-c4c1-0bb6a3208d29"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121ed48f",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Let's create two data points: a house with 1000 sqft sold for \\$300,000 and a house with 2000 sqft sold for \\$500,000.\n",
    "\n",
    "| Size (1000 sqft)     | Price (1000s of dollars) |\n",
    "| ----------------| ------------------------ |\n",
    "| 1               | 300                      |\n",
    "| 2               | 500                      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75428a4e-f708-4a61-9a56-288af4599b4a",
   "metadata": {
    "id": "75428a4e-f708-4a61-9a56-288af4599b4a"
   },
   "outputs": [],
   "source": [
    "# Create our data set\n",
    "x_train = np.array([1.0, 2.0])   #features\n",
    "y_train = np.array([300.0, 500.0])   #target value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba9a7b-7ee9-49fd-953a-e0fd1f7b2e0c",
   "metadata": {
    "id": "e3ba9a7b-7ee9-49fd-953a-e0fd1f7b2e0c"
   },
   "source": [
    "\n",
    "## Refresher on linear regression\n",
    "In this practice lab, you will fit the linear regression parameters $(w,b)$ to your datasetas\n",
    "\n",
    "- The model function for linear regression, which is a function that maps from `x` (size of a house) to `y` (price of the house) is represented as  \n",
    "    $$f_{w,b}(x) = wx + b$$\n",
    "    \n",
    "\n",
    "- To train a linear regression model, you want to find the best $(w,b)$ parameters that fit your dataset.  \n",
    "\n",
    "    - To compare how one choice of $(w,b)$ is better or worse than another choice, you can evaluate it with a cost function $J(w,b)$\n",
    "      - $J$ is a function of $(w,b)$. That is, the value of the cost $J(w,b)$ depends on the value of $(w,b)$.\n",
    "  \n",
    "    - The choice of $(w,b)$ that fits your data the best is the one that has the smallest cost $J(w,b)$.\n",
    "\n",
    "\n",
    "- To find the values $(w,b)$ that gets the smallest possible cost $J(w,b)$, you can use a method called **gradient descent**.\n",
    "  - With each step of gradient descent, your parameters $(w,b)$ come closer to the optimal values that will achieve the lowest cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f50f7f-ca69-423c-b094-6f55f9c723fe",
   "metadata": {
    "id": "85f50f7f-ca69-423c-b094-6f55f9c723fe"
   },
   "source": [
    "Your goal is to build a linear regression model to fit this data.\n",
    "- With this model, you can input the square footage of a new house, and it will estimate the potential selling price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817cde90-255a-4bf0-945a-a904c5008e84",
   "metadata": {
    "id": "817cde90-255a-4bf0-945a-a904c5008e84"
   },
   "source": [
    "## Implement Gradient Descent\n",
    "You will implement gradient descent algorithm for only one feature in this lab. Specifically, you will need three functions.\n",
    "- `compute_cost`\n",
    "- `compute_gradient`\n",
    "- `gradient_descent`\n",
    "\n",
    "Conventions:\n",
    "- The naming of python variables representing partial derivatives follows this pattern,$\\frac{\\partial J(w,b)}{\\partial b}$  will be denoted as `dj_db`.\n",
    "- w.r.t is With Respect To, as in partial derivative of $J(w,b)$ With Respect To $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25721a0b-9372-4a21-98cf-1c6d723edf9d",
   "metadata": {
    "id": "25721a0b-9372-4a21-98cf-1c6d723edf9d"
   },
   "source": [
    "### Compute_Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7777fd-33b5-43ef-b973-3bcd3f89141b",
   "metadata": {
    "id": "3c7777fd-33b5-43ef-b973-3bcd3f89141b"
   },
   "source": [
    "\n",
    "\n",
    "Gradient descent involves repeated steps to adjust the value of your parameter $(w,b)$ to gradually get a smaller and smaller cost $J(w,b)$.\n",
    "- At each step of gradient descent, it will be helpful for you to monitor your progress by computing the cost $J(w,b)$ as $(w,b)$ gets updated.\n",
    "- In this section, you will implement a function to calculate $J(w,b)$ so that you can check the progress of your gradient descent implementation.\n",
    "\n",
    "#### Cost function\n",
    "As you may recall from the lecture, for one variable, the cost function for linear regression $J(w,b)$ is defined as\n",
    "\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "- You can think of $f_{w,b}(x^{(i)})$ as the model's prediction of a house's selling price, as opposed to $y^{(i)}$, which is the actual sold price recorded in the data.\n",
    "- $m$ is the number of training examples in the dataset\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Please complete the `compute_cost()` function below to compute the cost $J(w,b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e4d34b-5f35-4973-b85f-26c3e5ab5f85",
   "metadata": {
    "id": "b0e4d34b-5f35-4973-b85f-26c3e5ab5f85"
   },
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7ef9ec-583e-4086-b94b-78c55857a7af",
   "metadata": {
    "id": "5c7ef9ec-583e-4086-b94b-78c55857a7af"
   },
   "source": [
    "\n",
    "\n",
    "Complete the `compute_cost` using for loops below to:\n",
    "\n",
    "* Iterate over the training examples, and for each example, compute:\n",
    "    * The prediction of the model for that example\n",
    "    $$\n",
    "    f_{wb}(x^{(i)}) =  wx^{(i)} + b\n",
    "    $$\n",
    "   \n",
    "    * The cost for that example  $$cost^{(i)} =  (f_{wb} - y^{(i)})^2$$\n",
    "    \n",
    "\n",
    "* Return the total cost over all examples\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} cost^{(i)}$$\n",
    "  * Here, $m$ is the number of training examples and $\\sum$ denotes the summation of the individual costs over all examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9aed6956-9545-48f9-8d7a-644c1cc1d35e",
   "metadata": {
    "id": "9aed6956-9545-48f9-8d7a-644c1cc1d35e"
   },
   "outputs": [],
   "source": [
    "# compute_cost\n",
    "\n",
    "def compute_cost(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): Shape (m,) Input to the model (Population of cities)\n",
    "        y (ndarray): Shape (m,) Label (Actual profits for the cities)\n",
    "        w, b (scalar): Parameters of the model\n",
    "\n",
    "    Returns\n",
    "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
    "               to fit the data points in x and y\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m = x.shape[0]\n",
    "\n",
    "    # You need to return this variable correctly\n",
    "    total_cost = 0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "   # Variable to keep track of sum of cost from each example\n",
    "    cost_sum = 0\n",
    "\n",
    "   # Loop over training examples\n",
    "    for i in range(m):\n",
    "       # Your code here to get the prediction f_wb for the ith example\n",
    "        f_wb =\n",
    "       # Your code here to get the cost associated with the ith example\n",
    "        cost =\n",
    "\n",
    "       # Your code here to add to sum of cost for each example\n",
    "        cost_sum =\n",
    "\n",
    "   # Your code here to get the total cost as the sum divided by (2*m)\n",
    "    total_cost =\n",
    "   ### END CODE HERE ###\n",
    "\n",
    "    return total_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad95f76-8295-423b-9337-594bc7fc8c73",
   "metadata": {
    "id": "dad95f76-8295-423b-9337-594bc7fc8c73"
   },
   "source": [
    "You can check if your implementation was correct by running the following test code: Please fill in the blanks in the Canvas quiz accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e4ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cost with some initial values for paramaters w, b\n",
    "initial_w = 2\n",
    "initial_b = 1\n",
    "\n",
    "cost = compute_cost(x_train, y_train, initial_w, initial_b)\n",
    "print(type(cost))\n",
    "print(f'Cost at initial w: {cost:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f42c7f",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Please complete the `compute_gradient` function to:\n",
    "\n",
    "* Iterate over the training examples, and for each example, compute:\n",
    "    * The prediction of the model for that example\n",
    "    $$\n",
    "    f_{wb}(x^{(i)}) =  wx^{(i)} + b\n",
    "    $$\n",
    "   \n",
    "    * The gradient for the parameters $w, b$ from that example\n",
    "        $$\n",
    "        \\frac{\\partial J(w,b)}{\\partial b}^{(i)}  =  (f_{w,b}(x^{(i)}) - y^{(i)})\n",
    "        $$\n",
    "        $$\n",
    "        \\frac{\\partial J(w,b)}{\\partial w}^{(i)}  =  (f_{w,b}(x^{(i)}) -y^{(i)})x^{(i)}\n",
    "        $$\n",
    "    \n",
    "\n",
    "* Return the total gradient update from all the examples\n",
    "    $$\n",
    "    \\frac{\\partial J(w,b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} \\frac{\\partial J(w,b)}{\\partial b}^{(i)}\n",
    "    $$\n",
    "    \n",
    "    $$\n",
    "    \\frac{\\partial J(w,b)}{\\partial w}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} \\frac{\\partial J(w,b)}{\\partial w}^{(i)}\n",
    "    $$\n",
    "  * Here, $m$ is the number of training examples and $\\sum$ is the summation operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64ddb976-e672-4c7c-bf17-7f5f3b9b459f",
   "metadata": {
    "id": "64ddb976-e672-4c7c-bf17-7f5f3b9b459f"
   },
   "outputs": [],
   "source": [
    "# compute_gradient\n",
    "def compute_gradient(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (m,) Input to the model (Population of cities)\n",
    "      y (ndarray): Shape (m,) Label (Actual profits for the cities)\n",
    "      w, b (scalar): Parameters of the model\n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b\n",
    "     \"\"\"\n",
    "\n",
    "    # Number of training examples\n",
    "    m = x.shape[0]\n",
    "\n",
    "    # You need to return the following variables correctly\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Loop over examples\n",
    "    for i in range (m):\n",
    "        # Your code here to get prediction f_wb for the ith example\n",
    "        f_wb =\n",
    "\n",
    "        # Your code here to get the gradient for w from the ith example\n",
    "        dj_dw_i =\n",
    "\n",
    "        # Your code here to get the gradient for b from the ith example\n",
    "        dj_db_i =\n",
    "\n",
    "        # Your code here to update dj_db\n",
    "        dj_db =\n",
    "\n",
    "        # Your code here to Update dj_dw\n",
    "        dj_dw =\n",
    "\n",
    "    dj_dw = \n",
    "    dj_db = \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e24e3-1fce-469f-97ef-9a63a9ca9254",
   "metadata": {
    "id": "d58e24e3-1fce-469f-97ef-9a63a9ca9254"
   },
   "source": [
    "Now let's run the gradient descent algorithm implemented above on our dataset. Please fill in the blanks in the Canvas quiz accordingly</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8cd99-37b3-46e8-bad9-c8194fadf8a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33a8cd99-37b3-46e8-bad9-c8194fadf8a9",
    "outputId": "41ef6c10-376f-4d7f-a8a6-b7eac22013a2"
   },
   "outputs": [],
   "source": [
    "# Compute and display gradient with w and b initialized to zeroes\n",
    "initial_w = 0\n",
    "initial_b = 0\n",
    "\n",
    "tmp_dj_dw, tmp_dj_db = compute_gradient(x_train, y_train, initial_w, initial_b)\n",
    "print('Gradient at initial w, b (zeros):', tmp_dj_dw, tmp_dj_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd72301-b5b2-4cc3-8c46-3ee8db914400",
   "metadata": {
    "id": "1bd72301-b5b2-4cc3-8c46-3ee8db914400"
   },
   "source": [
    "### Task 3:\n",
    "\n",
    "Please complete the `gradient_descent` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddf8d754-3045-4492-908b-9f74df6c70fd",
   "metadata": {
    "id": "ddf8d754-3045-4492-908b-9f74df6c70fd"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking\n",
    "    num_iters gradient steps with learning rate alpha\n",
    "\n",
    "    Args:\n",
    "      x :    (ndarray): Shape (m,)\n",
    "      y :    (ndarray): Shape (m,)\n",
    "      w_in, b_in : (scalar) Initial values of parameters of the model\n",
    "      cost_function: function to compute cost\n",
    "      gradient_function: function to compute the gradient\n",
    "      alpha : (float) Learning rate\n",
    "      num_iters : (int) number of iterations to run gradient descent\n",
    "    Returns\n",
    "      w : (ndarray): Shape (1,) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "      b : (scalar)                Updated value of parameter of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "\n",
    "    # number of training examples\n",
    "    m = len(x)\n",
    "\n",
    "    # An array to store cost J and w's at each iteration â€” primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    ### START CODE HERE ###\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Your code here to calculate the gradient and update the parameters\n",
    "        dj_dw, dj_db =\n",
    "\n",
    "        # Your code here to update Parameters using w, b, alpha and gradient\n",
    "        w =\n",
    "        b =\n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion\n",
    "            cost =  cost_function(x, y, w, b)\n",
    "            J_history.append(cost)\n",
    "            w_history.append([w,b])\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "\n",
    "            # print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    "\n",
    "    return w, b, J_history, w_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee48d490-3ec7-4ade-8c81-5fc561aadc0f",
   "metadata": {
    "id": "ee48d490-3ec7-4ade-8c81-5fc561aadc0f"
   },
   "source": [
    "### Congrats, you are done with the gradient descent implementaton\n",
    "Below, Let's run the gradient descent algorithm above to to find optimal values of $w$ and $b$ on the training data, Please fill in the blanks in the Canvas quiz accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0951a1-073d-4895-9bb1-3b7668e2cde3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e0951a1-073d-4895-9bb1-3b7668e2cde3",
    "outputId": "22154047-5730-4eac-a25a-d887ab7ed8a4"
   },
   "outputs": [],
   "source": [
    "# initialize fitting parameters. Recall that the shape of w is (n,)\n",
    "initial_w = 0.\n",
    "initial_b = 0.\n",
    "\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.01\n",
    "\n",
    "w, b, J_hist, p_hist = gradient_descent(x_train ,y_train, initial_w, initial_b,\n",
    "                     compute_cost, compute_gradient, alpha, iterations)\n",
    "print(\"w,b found by gradient descent:\", w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b7112-7a06-481e-8daf-f0852fd885ec",
   "metadata": {
    "id": "867b7112-7a06-481e-8daf-f0852fd885ec"
   },
   "source": [
    "### Learning Curve\n",
    "\n",
    "Let's now visualize the learning curve, observing how the cost function evolves with each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41184459-21ed-4bbb-a371-24409929c056",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "41184459-21ed-4bbb-a371-24409929c056",
    "outputId": "993fe0ea-76e5-4126-851b-0571db16486c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot cost versus iteration\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
    "ax1.plot(J_hist[:100])\n",
    "ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])\n",
    "ax1.set_title(\"Cost vs. iteration(start)\");  ax2.set_title(\"Cost vs. iteration (end)\")\n",
    "ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost')\n",
    "ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a840ee-f302-4c1d-8f31-b79518f24ee7",
   "metadata": {
    "id": "90a840ee-f302-4c1d-8f31-b79518f24ee7"
   },
   "source": [
    "## Model Prediction\n",
    "\n",
    "### Task 4:\n",
    "\n",
    "- For linear regression with one variable, the prediction of the model $f_{w,b}$ for an example $x^{(i)}$ is representented as:\n",
    "\n",
    "$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b$$\n",
    "\n",
    "This is the equation for a line, with an intercept $b$ and a slope $w$\n",
    "\n",
    "With the optimal values for the parameters $w$ and $b$, you can now use the model to predict housing values based on our learned parameters. Please make precition for a house with 1200 sqft below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SF2ScjY1RYJ8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF2ScjY1RYJ8",
    "outputId": "6c2211ce-6bba-4eae-c560-ad7e3203d795"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "\n",
    "### End CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344729aa-f0e6-4910-b08a-c0c667c964fc",
   "metadata": {
    "id": "344729aa-f0e6-4910-b08a-c0c667c964fc"
   },
   "source": [
    "\n",
    "## Congratulations!\n",
    "In this lab you:\n",
    "- delved into the details of gradient descent for a single variable.\n",
    "- implement the gradient descent algorithm using python for loops.\n",
    "- utilized gradient descent to find optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcfdab3-e6d8-426f-8699-cd01f763d538",
   "metadata": {
    "id": "1dcfdab3-e6d8-426f-8699-cd01f763d538"
   },
   "source": [
    "## Reference:\n",
    "https://www.deeplearning.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f328376-71d5-46af-883d-06d9feba7383",
   "metadata": {
    "id": "0f328376-71d5-46af-883d-06d9feba7383"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
